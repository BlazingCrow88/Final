# ===================================
# ROBOTS.TXT
# File: robots.txt
# ===================================

# Allow all web crawlers access to all content
User-agent: *
Allow: /

# Disallow access to admin areas
Disallow: /admin/
Disallow: /api/
Disallow: /assets/admin/

# Disallow temporary and system files
Disallow: /tmp/
Disallow: /.git/
Disallow: /.env
Disallow: /config/
Disallow: /vendor/
Disallow: /node_modules/

# Disallow checkout and cart pages (no SEO value)
Disallow: /checkout.html
Disallow: /cart.html
Disallow: /order-success.html

# Disallow search results and filtered pages
Disallow: /search?*
Disallow: /*?search=*
Disallow: /*?filter=*
Disallow: /*?page=*

# Disallow user-specific pages
Disallow: /my-account/
Disallow: /wishlist/
Disallow: /compare/

# Allow but crawl delay for resource-heavy pages
User-agent: *
Crawl-delay: 1

# Specific rules for major search engines
User-agent: Googlebot
Allow: /
Crawl-delay: 0

User-agent: Bingbot
Allow: /
Crawl-delay: 1

User-agent: Slurp
Allow: /
Crawl-delay: 2

# Block bad bots and scrapers
User-agent: AhrefsBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: Exabot
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: GPTBot
Disallow: /

User-agent: Google-Extended
Disallow: /

# Allow access to specific assets
Allow: /assets/css/
Allow: /assets/js/
Allow: /assets/images/
Allow: /assets/fonts/

# Sitemap location
Sitemap: https://authorname.com/sitemap.xml
Sitemap: https://authorname.com/sitemap-images.xml
Sitemap: https://authorname.com/sitemap-blog.xml

# Host specification (helps with canonicalization)
Host: https://authorname.com
